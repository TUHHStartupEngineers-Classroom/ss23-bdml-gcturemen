---
title: "Explaining Black-Box Models with LIME"
author: "Gani Cem Türemen"
---

# Challenge Summary

In this challenge, we take the models we’ve developed and explain their localized prediction results to understand the reasons behind their predictions which will help with the assessment of trust to a model. 

# Objectives

Apply your learning of LIME (Local Interpretable Model-Agnostic Explanations) on H2O models to explain their results.

# Solution

## Import Libraries

```{r}
#| output: false

library(h2o)
library(recipes)
library(readxl)
library(tidyverse)
library(tidyquant)
library(lime)
```

## Load the training & test dataset

```{r}
#| output: false

employee_attrition_tbl <- read_csv(
  "data/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv")

definitions_raw_tbl <- read_excel("data/data_definitions.xlsx",
                                  sheet = 1,
                                  col_names = FALSE)
```

```{r}
#| include: false

# import data processing pipeline
source("data/data_processing_pipeline.R")
```

Convert the data into human readable form using the function `process_hr_data_readable()` from the lecture:
```{r}
#| output: false

employee_attrition_readable_tbl <- process_hr_data_readable(employee_attrition_tbl, 
                                                            definitions_raw_tbl)

employee_attrition_readable_tbl %>% glimpse()
```

```{r}
#| echo: false

employee_attrition_readable_tbl %>% glimpse()
```

Split the data into training and test sets:
```{r}
set.seed(seed = 1113)
split_obj <- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)

train_readable_tbl <- rsample::training(split_obj)
test_readable_tbl  <- rsample::testing(split_obj)
```

Columns `JobLevel` and `StockOptionLevel` are numeric but they represent levels so they have to be converted into factor. The recipe below will take care of that:
```{r}
recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_mutate_at(c("JobLevel", "StockOptionLevel"), fn = as.factor) %>% 
  prep()

train_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
test_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)

train_tbl %>% glimpse()
```

## Specify the response and predictor variables

To determine the attrition status `Attrition`, all the other columns will be used as predictors:
```{r}
#| output: false

h2o.init()

split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.80), seed = 767)
train_h2o <- split_h2o[[1]]
valid_h2o <- split_h2o[[2]]
test_h2o  <- as.h2o(test_tbl)

y <- "Attrition"
x <- setdiff(names(train_h2o), y)
```

## Run AutoML

Set 30 seconds for `max_runtime_secs` and 5 for `nfolds`:
```{r}
#| output: false

automl_models_h2o <- h2o.automl(
  x = x,
  y = y,
  training_frame    = train_h2o,
  validation_frame  = valid_h2o,
  leaderboard_frame = test_h2o,
  max_runtime_secs  = 30,
  nfolds            = 5 
)
```

## View the leaderboard

```{r}
automl_models_h2o@leaderboard
```

## Predicting using Leader Model

```{r}
#| include: false

# Extracts and H2O model name by a position so can more easily use h2o.getModel()
extract_h2o_model_name_by_position <- function(h2o_leaderboard, n = 1, verbose = T) {
    
    model_name <- h2o_leaderboard %>%
        as_tibble() %>%
        slice(n) %>%
        pull(model_id)
    
    if (verbose) message(model_name)
    
    return(model_name)
    
}
```

Extract the first model of the leaderboard:
```{r}
leader_model <- automl_models_h2o@leaderboard %>% 
  extract_h2o_model_name_by_position(1) %>% 
  h2o.getModel()
```

```{r}
#| include: false

leader_model %>% h2o.saveModel(path = "h2o_models/")
```

Use the extracted model to make predictions on the test set:
```{r}
#| output: false

predictions_tbl <- leader_model %>% 
  h2o.predict(newdata = as.h2o(test_tbl)) %>%
  as_tibble() %>%
  bind_cols(test_tbl %>% select(Attrition, EmployeeNumber)
  )

predictions_tbl %>% glimpse()
```

```{r}
#| echo: false

predictions_tbl %>% glimpse()
```

## Create Explainer

Specify the explainer which will later be used to create explanations. Since `Attrition` column is not a predictor, it will be removed:
```{r}
explainer <- train_tbl %>%
  select(-Attrition) %>%
  lime(
    model           = leader_model,
    bin_continuous  = TRUE,
    n_bins          = 4,
    quantile_bins   = TRUE
  )
```

## Explanation of Single Case

Create a single explanation for the second case in `test_tbl`. Again remove the `Attrition` column since it is not a predictor:
```{r}
#| output: false

explanation <- test_tbl %>%
  slice(2) %>%
  select(-Attrition) %>%
  lime::explain(explainer = explainer, 
                n_labels   = 1, 
                n_features = 8, 
                n_permutations = 5000,
                kernel_width = 0.5)

explanation
```

```{r}
#| echo: false

explanation
```

Create a custom function `custom_plot_features()` to visualize feature importance for a given single explanation:
```{r}
custom_plot_features <- function(expl) {
  
  case <- expl$case[1]
  lab <- expl$label[1]
  prob <- expl$label_prob[1] %>% round(2)
  expl_fit <- expl$model_r2[1] %>% round(2)
  
  text <- paste(" Case: ", case, "\n", 
                "Label: ", lab, "\n", 
                "Probability: ", prob, "\n", 
                "Explanation Fit: ", expl_fit)
  
  g <- expl %>% 
    arrange(desc(abs(feature_weight))) %>%
    mutate(feature_type = case_when((feature_weight) >= 0 ~ "Supports", 
                                    TRUE ~ "Contradicts") %>% as.factor()) %>%
    
    ggplot(aes(x = feature_desc, y = feature_weight, group = feature_weight)) +
    geom_col(aes(reorder(feature_desc, abs(feature_weight)), fill = feature_type)) + 
    scale_fill_manual("", values = c("Supports" = "#FF8014", "Contradicts" = "#2DC6D6")) +
    coord_flip() +
    labs(x = "Feature", y = "Weight", subtitle = text) +
    theme(
      legend.position = "bottom", 
      legend.background = element_rect(fill = NA, color = "white"),
      legend.key = element_blank(),
      panel.background = element_blank(), 
      panel.border = element_rect(color = "white", fill = NA), 
      plot.background = element_rect(fill = "#222222", color = "#222222"), 
      line = element_line(color = "white"), 
      text = element_text(color = "white"), 
      axis.ticks = element_line(color = "white"), 
      axis.text = element_text(color = "white"),
      plot.subtitle = element_text(size = 7)
  )
    
  return(g)
}
```

Visualize the explanation of the single case using `custom_plot_features()`:
```{r}
custom_plot_features(explanation)
```

## Explanation of Multiple Cases

Create an explanation for the first 20 cases in `test_tbl`. Again remove the `Attrition` column since it is not a predictor:
```{r}
#| output: false

explanation_multiple <- test_tbl %>%
  slice(1:20) %>%
  select(-Attrition) %>%
  lime::explain(
    explainer = explainer,
    n_labels   = 1,
    n_features = 8,
    n_permutations = 5000,
    kernel_width   = 0.5
  )

explanation_multiple
```

```{r}
#| echo: false

explanation_multiple
```

Create a custom function `custom_plot_explanations()` to visualize feature importance for multiple cases in a given explanation:
```{r}
custom_plot_explanations <- function(expl) {
  
  expl$case <- as_factor(expl$case)
  
  g <- expl %>%
    ggplot(aes(x = case, y = feature_desc, fill = feature_weight)) +
    geom_tile() + 
    facet_wrap(~ label) +
    scale_fill_gradient2("Feature \nWeight", 
                         low = "#2DC6D6", 
                         high = "#FF8014", 
                         mid = "white") + 
    labs(x = "Case", y = "Feature") +
    theme(
      legend.background = element_rect(fill = NA, color = "white"), 
      legend.key = element_blank(),
      panel.background = element_blank(),
      panel.border = element_rect(color = "white", fill = NA),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      plot.background = element_rect(fill = "#222222", color = "#222222"), 
      line = element_line(color = "white"), 
      text = element_text(color = "white"), 
      axis.ticks = element_line(color = "white"), 
      axis.text = element_text(color = "white", size = 5)
  )
  
  return(g)
}
```

Visualize the explanation of multiple cases using `custom_plot_explanations()`:
```{r}
custom_plot_explanations(explanation_multiple)
```

```{r}
#| include: false

h2o.shutdown()
```